# py -m install nltk
import nltk

# nltk.download('punkt')
# nltk.download('all')

# sample text
text="""Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.
The sky is pinkish-blue. You shouldn't eat cardboard"""

# tokenizing the sentence
from nltk.tokenize import sent_tokenize

tokenized = sent_tokenize(text)
print(tokenized)

# tokenizing the words
from nltk.tokenize import word_tokenize

tokenized_words = word_tokenize(text)
print(tokenized_words)

# nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords = set(stopwords.words('english'))

print(stopwords)

# removing stopwords 
filtered_words = []
for w in tokenized_words:
    if w not in stopwords:
        filtered_words.append(w)
print(filtered_words)

# stemming 
from nltk.stem import PorterStemmer
ps = PorterStemmer()
stemmed_words=[]
for w in filtered_words:
    stemmed_words.append(ps.stem(w))
print("Stemmed Words:",stemmed_words)

# nltk.download('wordnet')

# Lemmatization
from nltk.stem.wordnet import WordNetLemmatizer
lem = WordNetLemmatizer()
lemmatized_words = []
for w in filtered_words:
    lemmatized_words.append(lem.lemmatize(w))
print("lemmatized Sentence: ", lemmatized_words)

# Pos tagging (part of speech)

# !py -m pip install spacy
import spacy

# ! py -m spacy download en_core_web_sm

nlp = spacy.load('en_core_web_sm')
doc = nlp(text)

for token in doc:
    print(token, "|", token.pos_,"|", spacy.explain(token.pos_),"|",token.tag_, spacy.explain(token.tag_))
    print("")

# Term Frequency and Inverse Document Frequency
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer()
result = tfidf.fit_transform(tokenized_words)
print("TF-IDF Values : \n",result)

# in matrix form
print('\ntf-idf values in matrix form:')
print(result.toarray())
print('\nWord indexes:')
print(tfidf.vocabulary_,"\n")